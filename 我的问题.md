1. variation rate 具体的周期是多少
2. scale后怎么放大回原来的尺度
3. 训练了多少轮？
4. 用了多少条数据？
5. 学习率怎么调？？？
   
   我的RNN是一条直线，MLPANN一开始也是
6. 权重具体怎么求？

<br/>

***
# How can I avoid underfitting in Pytorch NeuralNetwork

As a beginner of ML, I try to predict the power consumtion of a plant based on seven features. I have built two simple neural network models. 
The **first** one is a **Linear** model, and the **second** is a **RNN** model. However, both models perform bad in the test set, their forecast result is a **straight line**.

#### Something about data

There are about 360 samples in the CSV file. I take the first 300 samples for trainning and the others for test. The first 7 columns of raw data are features of daily operation. The last column is the electricity consumption of every day. 

#### Setting of training set

- In the **linear** model, train data is the first 7 colums of a certain day, and corresponding target is the power consumption of that day.
- In the **RNN** model, train data is all the 8 columns of three days(seven features and power consumption), and corresponding traget is the power consumption of next three days.

***

## Solutions and its Effect

### I have looked around and apparently I've got the choice between these solutions:

- Add new domain-specific features
- Decrease the amount of regularization used
- Increase the duration of training
- Increase the complexity or type of the model
- Decrease the learning rate
- Try other activate function

### I have tried some solutions:

- The data for trainning isn't regularized. I just change the unit of electricity from kWh to Wh 
- I take *ReLu* as activate function after using *Sigmoid*, but it doesn't work
- I adjust the learning rate from **0.01** to **0.001**, it doesn't improve
- I try different optimizer such as *SGD* and *Adam* on both model, even use *momentum*, it doesn't get better
- The sequence length of RNN model is 60 firstly, then is set to 3. The loss dropped more rapidly in the latter case, but the forecast result still is a **straight line**

In a word, all solutions I find doesn't work.

Besides, if `shuffle` is `True` when building DataLoader, the loss skips violently between epochs. But it drops slowly and close to an constant eventually when `shuffle` is `False`.

***

## Code

### Code of RNN model

```
import torch
import pandas as pd
import numpy as np
import torch.nn.functional as f
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from matplotlib import pyplot as plt

'''
build simple RNN
'''

batchSize = 3
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
netPath = ''

'''Data processing'''
# read raw data
filePath = 'F:/.csv'
initialData = pd.read_csv(filePath)
print(initialData.head(10))
print('hello world')
# Separate features and power consumption.
trainDatas = initialData.iloc[0:7, 1:301]
trainPowerConsum = pd.DataFrame(initialData.iloc[-1, 1:301]).T
trainDatas = pd.concat([trainDatas, trainPowerConsum], 0)

trainPowerConsum = initialData.iloc[-1, 2:302]
# Plot
powerConsumPlot = trainDatas.iloc[-1, :]
xData = np.linspace(1, powerConsumPlot.shape[0], 300)
plt.plot(xData, powerConsumPlot)
plt.show()

testDatas = initialData.iloc[0:7, 302:-1]
testPowerConsum = pd.DataFrame(initialData.iloc[-1, 302:-1]).T
testDatas = pd.concat([testDatas, testPowerConsum], 0)
testPowerConsum = initialData.iloc[-1, 303:]

# convert to dataframe
trainDatas = pd.DataFrame(trainDatas)
trainDatas = trainDatas.T

trainPowerConsum = pd.DataFrame(trainPowerConsum)

testDatas = pd.DataFrame(testDatas)
testDatas = testDatas.T

testPowerConsum = pd.DataFrame(testPowerConsum)

# change the unit of PowerConsumption
trainDatas.iloc[:, -1] = trainDatas.iloc[:, -1] * 1000
testDatas.iloc[:, -1] = testDatas.iloc[:, -1] * 1000
trainPowerConsum.iloc[:, 0] = trainPowerConsum.iloc[:, 0] * 1000
testPowerConsum.iloc[:, 0] = testPowerConsum.iloc[:, 0] * 1000

assert testPowerConsum.shape[0] == testDatas.shape[0]
assert trainDatas.shape[0] == trainPowerConsum.shape[0]

# convert dataframe to tensor
trainDatas = torch.tensor(trainDatas.values.astype(float), device=device)
trainPowerConsum = torch.tensor(trainPowerConsum.values.astype(float), device=device)
testDatas = torch.tensor(testDatas.values.astype(float), device=device)
testPowerConsum = torch.tensor(testPowerConsum.values.astype(float), device=device)

trainDatasList = list()
trainPowerConsumList = list()
for i in range(298):
    trainDatasList.append(trainDatas[i:i + 3])
    trainPowerConsumList.append(trainPowerConsum[i:i + 3])

from torch.nn.utils.rnn import pad_sequence
trainPowerConsum = pad_sequence(trainPowerConsumList, batch_first=True)
trainDatas = pad_sequence(trainDatasList, batch_first=True)
print(trainDatas.shape)

# ensure the batch_size of test data is 1
testDatas = torch.unsqueeze(testDatas, dim=0)
testPowerConsum = torch.unsqueeze(testPowerConsum, dim=0)

'''build dataloader'''
trainDataLoader = DataLoader(
    TensorDataset(
        trainDatas, trainPowerConsum
    ),
    shuffle=True, batch_size=batchSize, drop_last=True)
print('Data is ready')

seqLen = 2
inputDim = 8
hiddenSize = 3
numLayer = 2
learningRate = 0.01


class RNNModel(torch.nn.Module):
    def __init__(self, inputsize, hiddensize, batchsize, numLayer):
        super(RNNModel, self).__init__()
        self.batchsize = batchsize
        self.inputsize = inputsize
        self.hiddensize = hiddensize
        self.numlayers = numLayer
        self.rnn = torch.nn.RNN(input_size=self.inputsize, hidden_size=self.hiddensize, num_layers=self.numlayers,
                                batch_first=True)
        self.l1 = torch.nn.Linear(hiddenSize, hiddensize)
        self.l2 = torch.nn.Linear(hiddenSize, 1)

    def forward(self, input, hidden):
        out, hidden = self.rnn(input.float(), hidden.float())
        batch_size, seq_len, input_dim = out.shape
        out = out.reshape(-1, input_dim)
        # out = f.sigmoid(self.l1(out))
        out = f.relu(self.l1(out))
        out = self.l2(out)
        out = out.reshape(batch_size, seq_len, -1)

        return out, hidden

    def initHidden(self):
        hidden = torch.zeros(self.numlayers, self.batchsize, self.hiddensize, device=device, dtype=torch.float64)
        return hidden


net = RNNModel(inputDim, hiddenSize, batchSize, numLayer).to(device)
criterion = torch.nn.L1Loss()  
optimizer = optim.Adam(net.parameters(), lr=learningRate,momentum=0.01)


def train(epoch):
    runLoss = 0.
    optimizer.zero_grad()
    hidden = net.initHidden()

    for batchIndex, data in enumerate(trainDataLoader, 0):
        inputs, target = data
        optimizer.zero_grad()
        outputs, hidden = net(inputs, hidden)
        hidden = hidden.detach()  
        loss = criterion(outputs.float(), target.float())
        loss = loss.mean()

        loss.backward()
        optimizer.step()

    print(f'{epoch + 1},\t Loss={loss.item()}')
    # torch.save(net.state_dict(), netPath)


def test():
    testDatasVice = torch.clone(testDatas)
    input = testDatasVice[:, 0, :]
    input = input.view(1, 1, -1)
    assert input.shape[2] == 8
    predictPowConsum = list()
    # the first hidden tensor in test set is zero
    hidden = torch.zeros(2, 1, 3, device=device, dtype=torch.float64)
    with torch.no_grad():
        for i in range(testDatas.shape[1]):
            output, hidden = net(input, hidden)
            if i < 51:
                testDatasVice[:, i + 1, -1] = output[0]
                input = torch.unsqueeze(testDatasVice[:, i + 1, :], dim=0)
                predictPowConsum.append(output.data.cpu().numpy().ravel()[0])
            elif i == 51:
                predictPowConsum.append(output.data.cpu().numpy().ravel()[0])
            else:
                print('\tindexError')  # Exclude potential Errors
    return predictPowConsum


if __name__ == '__main__':
    epochNum = 300
    for epoch in range(epochNum):
        train(epoch)
    predictPowConsum = test()
    # plotting
    xData = np.arange(303, 303 + testPowerConsum.size(1))
    plt.plot(xData, testPowerConsum.cpu().numpy()[0, :, 0])
    plt.plot(xData, predictPowConsum)
    plt.show()
```

### Code of Linear model

```
import torch
import pandas as pd
import numpy as np
import torch.nn.functional as f
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from matplotlib import pyplot as plt

filePath = 'F:.csv'
initialData = pd.read_csv(filePath)
print(initialData.head(10))
print('hello world')

trainDatas = initialData.iloc[0:7, 1:300]
trainPowerConsum = initialData.iloc[-1, 1:300]
testDatas = initialData.iloc[0:7, 300:-1]
testPowerConsum = initialData.iloc[-1, 300:-1]

trainDatas = pd.DataFrame(trainDatas)
trainDatas = trainDatas.T

trainPowerConsum = pd.DataFrame(trainPowerConsum)

testDatas = pd.DataFrame(testDatas)
testDatas = testDatas.T

testPowerConsum = pd.DataFrame(testPowerConsum)

trainPowerConsum.iloc[:, 0] = trainPowerConsum.iloc[:, 0] * 1000
testPowerConsum.iloc[:, 0] = testPowerConsum.iloc[:, 0] * 1000

# build dataloader
trainData = DataLoader(
    TensorDataset(
        torch.tensor(trainDatas.values).float(),
        torch.tensor(trainPowerConsum.values.astype(float)).float()
    ),
    shuffle=True, batch_size=15)

testData = DataLoader(
    TensorDataset(
        torch.tensor(testDatas.values.astype(float)).float(),
        torch.tensor(testPowerConsum.values.astype(float)).float()
    ),
    shuffle=False, batch_size=15)
print('data is ready')


class SimpleNet(torch.nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.l1 = torch.nn.Linear(7, 15)
        self.l2 = torch.nn.Linear(15, 30)
        self.l3 = torch.nn.Linear(30, 15)
        self.l4 = torch.nn.Linear(15, 5)
        self.l5 = torch.nn.Linear(5, 1)

    def forward(self, x):
        x = f.relu(self.l1(x))
        x = f.relu(self.l2(x))
        x = f.relu(self.l3(x))
        x = f.relu(self.l4(x))
        return self.l5(x)


model = SimpleNet()
criterion = torch.nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.0001)


def train(epoch):
    runLoss = 0.
    for batch_index, data in enumerate(trainData, 0):
        inputs, target = data
        optimizer.zero_grad()

        outputs = model(inputs)

        loss = criterion(outputs, target)
        loss.backward()
        optimizer.step()
        runLoss += loss
        print(f'{epoch + 1},{batch_index + 1},\tLoss={runLoss / 5}')
        runLoss = 0


def test(epoch):
    totalError = 0.
    print('Start to test the model')
    with torch.no_grad():
        for data in testData:
            # test ---------data for test
            # testlab ---------corresponding power consumption
            test, testlab = data
            outputs = model(test)
            predicted = outputs.data
            testError = testlab - predicted
            # plotting
            if epoch % 50 == 2:
                xData = np.linspace(1, 15, 15)
                if predicted.size(0) != 15:
                    pass
                else:
                    plt.plot(xData, predicted[:, 0].numpy(), label='predicted', color='red')
                    plt.plot(xData, testlab[:, 0].numpy(), label='origData', color='blue')
                    plt.show()

            totalError += (torch.abs(testError).sum().item())

        print(f'Average Error on test set is {totalError / 54}')


if __name__ == '__main__':
    for epoch in range(1000):
        train(epoch)
        test(epoch)
   
```

## Image of Output 

<br/>

The blue line is the actual data, and the orange line is the output of RNN model.

***

Since English isn't my mother tongue, my description may not so clear. Beg your pardon.

What could be the best way to avoid the problem? 

Thanks in advance!
